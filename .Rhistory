# Clear the R workspace/environment
rm(list = ls())
#load data
s_data=read.csv("Steel_industry_data.csv")
names(s_data)
# Display the dimensions (number of rows and columns) of the dataset.
dim(data)
#exclude load_type feature
data = s_data[, c("date","Usage_kWh" ,"Lagging_Current_Reactive.Power_kVarh", "Leading_Current_Reactive_Power_kVarh",
"CO2.tCO2.", "Lagging_Current_Power_Factor", "Leading_Current_Power_Factor" , "NSM", "WeekStatus",
"Day_of_week")]
sum(is.na(data))# Count the number of missing values in the "Salary" column.
str(data)
# Convert categorical features  to numerical.
data$WeekStatus=as.numeric(factor(data$WeekStatus))
setwd("C:/Users/kaman/OneDrive - The University of Memphis/Stat 2 Project/project files/project code")
# Clear the R workspace/environment
rm(list = ls())
#load data
s_data=read.csv("Steel_industry_data.csv")
names(s_data)
# Display the dimensions (number of rows and columns) of the dataset.
dim(data)
#exclude load_type feature
data = s_data[, c("date","Usage_kWh" ,"Lagging_Current_Reactive.Power_kVarh", "Leading_Current_Reactive_Power_kVarh",
"CO2.tCO2.", "Lagging_Current_Power_Factor", "Leading_Current_Power_Factor" , "NSM", "WeekStatus",
"Day_of_week")]
sum(is.na(data))# Count the number of missing values in the "Salary" column.
str(data)
# Convert categorical features  to numerical.
data$WeekStatus=as.numeric(factor(data$WeekStatus))
data$Day_of_week=as.numeric(factor(data$Day_of_week))
#check unique class for each numerical converted categorical features
unique(data$WeekStatus)
unique(data$Day_of_week)
# Create new varaible/feature "Month" based on "Date"
library(lubridate) # Load the lubridate package for date-time manipulation
# Convert the "date" column to a DateTime object
data$date = dmy_hm(data$date)  # Assumes "date" column contains character date-time values
# Extract the month and store it as a numeric variable
Month = month(data$date)
unique(Month)
# Remove the date feature
data$date = NULL
# Combine the original data with the Month
data = cbind(data, Month)
names(data)
#create scatter plots for energy consumption for each month, week status and day
par(mfrow = c(1, 3))
plot(data$Month,data$Usage_kWh,xlab="Month",ylab="Energy Consumption")
# Create a 70%-30% train-test split
set.seed(1)
test = sample(nrow(data), 0.3 * nrow(data))
# Specify train data
train = (1:nrow(data))[-test] #Select rest of the data i.e 70% other than test data as training data
train_data <- data[train, ]
test_data <- data[test, ]
train_output <- train_data$Usage_kWh
test_output <- test_data$Usage_kWh
# Load required library for Random Forest
library(randomForest)
# Specify the degree of the polynomial
degree <- 2
# Extract numeric predictors (excluding the response variable "Usage_kWh" and non-numeric columns)
numeric_predictors <- train_data[, sapply(train_data, is.numeric) & names(train_data) != "Usage_kWh"]
# Ensure all values in numeric predictors are numeric
numeric_predictors <- sapply(numeric_predictors, as.numeric)
# Check for missing values and replace them with 0
numeric_predictors[is.na(numeric_predictors)] <- 0
# Apply polynomial transformation to numeric predictors
poly_features_train <- as.data.frame(poly(numeric_predictors, degree, raw = TRUE))
# Combine the polynomial features with the original training data
train_data_poly <- cbind(train_data, poly_features_train)
# Train a Polynomial Regression model with the polynomial features
poly_LR_model <- lm(Usage_kWh ~ ., data = train_data_poly)
# Display a summary of the Polynomial Regression model
summary(poly_LR_model)
# Apply the same polynomial transformation to the test data
numeric_predictors_test <- test_data[, sapply(test_data, is.numeric) & names(test_data) != "Usage_kWh"]
numeric_predictors_test <- sapply(numeric_predictors_test, as.numeric)
numeric_predictors_test[is.na(numeric_predictors_test)] <- 0
poly_features_test <- as.data.frame(poly(numeric_predictors_test, degree, raw = TRUE))
test_data_poly <- cbind(test_data, poly_features_test)
# Make predictions on the test data using the polynomial model
predictions_poly <- predict(poly_model, newdata = test_data_poly)
# Evaluate the polynomial model using metrics (e.g., RMSE, R-squared)
mse_poly <- mean((test_output - predictions_poly)^2)
r_squared_poly <- 1 - (sum((test_output - predictions_poly)^2) / sum((test_output - mean(test_output))^2))
cat("Polynomial Regression Model (Using All Features):\n")
cat("Mean Squared Error (MSE):", mse_poly, "\n")
cat("R-squared:", r_squared_poly, "\n")
# Load required library for Random Forest
library(randomForest)
# Train a Random Forest model
# Assuming "train_data_poly" contains both original and polynomial features
# Define predictors and response variable
predictors_rf <- train_data_poly[, !(names(train_data_poly) %in% c("Usage_kWh"))]
response_rf <- train_data_poly$Usage_kWh
# Train the Random Forest model
rf_model_poly <- randomForest(x = predictors_rf, y = response_rf, ntree = 100)
# Apply the model to the test data
predict_test_rf <- predict(rf_model_poly, newdata = test_data_poly)
# Evaluate the Random Forest model using metrics (e.g., RMSE, R-squared)
mse_rf_poly <- mean((test_output - predict_test_rf)^2)
r_squared_rf_poly <- 1 - (sum((test_output - predict_test_rf)^2) / sum((test_output - mean(test_output))^2))
cat("Polynomial Random Forest Model:\n")
cat("Mean Squared Error (MSE) fpr polynomial Random Forest Model:", mse_rf_poly, "\n")
cat("R-squared for polynomial Random Forest Model:", r_squared_rf_poly, "\n")
